{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38ad6abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Dýrmundur Helgi R. Óskarsson\n",
    "# Date: 12.10.2023\n",
    "# Project: 08 Boosting\n",
    "# Acknowledgements: \n",
    "#\n",
    "\n",
    "# NOTE: Your code should NOT contain any main functions or code that is executed\n",
    "# automatically.  We ONLY want the functions as stated in the README.md.\n",
    "# Make sure to comment out or remove all unnecessary code before submitting.\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import (train_test_split, RandomizedSearchCV)\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import (confusion_matrix, accuracy_score, recall_score, precision_score)\n",
    "\n",
    "from tools import get_titanic, build_kaggle_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c88ab60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1.1\n",
    "\n",
    "def get_better_titanic():\n",
    "    '''\n",
    "    Loads the cleaned titanic dataset but change\n",
    "    how we handle the age column.\n",
    "    '''\n",
    "    # Load in the raw data\n",
    "    # check if data directory exists for Mimir submissions\n",
    "    # DO NOT REMOVE\n",
    "    if os.path.exists('./data/train.csv'):\n",
    "        train = pd.read_csv('./data/train.csv')\n",
    "        test = pd.read_csv('./data/test.csv')\n",
    "    else:\n",
    "        train = pd.read_csv('train.csv')\n",
    "        test = pd.read_csv('test.csv')\n",
    "\n",
    "    # Concatenate the train and test set into a single dataframe\n",
    "    # we drop the `Survived` column from the train set\n",
    "    X_full = pd.concat([train.drop('Survived', axis=1), test], axis=0)\n",
    "    \n",
    "    median_age = X_full['Age'].median()\n",
    "    X_full['Age'].fillna(median_age, inplace=True)\n",
    "\n",
    "    # The cabin category consist of a letter and a number.\n",
    "    # We can divide the cabin category by extracting the first\n",
    "    # letter and use that to create a new category. So before we\n",
    "    # drop the `Cabin` column we extract these values\n",
    "    X_full['Cabin_mapped'] = X_full['Cabin'].astype(str).str[0]\n",
    "    # Then we transform the letters into numbers\n",
    "    cabin_dict = {k: i for i, k in enumerate(X_full.Cabin_mapped.unique())}\n",
    "    X_full.loc[:, 'Cabin_mapped'] =\\\n",
    "        X_full.loc[:, 'Cabin_mapped'].map(cabin_dict)\n",
    "\n",
    "    # We drop multiple columns that contain a lot of NaN values\n",
    "    # in this assignment\n",
    "    # Maybe we should\n",
    "    X_full.drop(\n",
    "        ['PassengerId', 'Cabin', 'Name', 'Ticket'],\n",
    "        inplace=True, axis=1)\n",
    "\n",
    "    # Instead of dropping the fare column we replace NaN values\n",
    "    # with the 3rd class passenger fare mean.\n",
    "    fare_mean = X_full[X_full.Pclass == 3].Fare.mean()\n",
    "    X_full['Fare'].fillna(fare_mean, inplace=True)\n",
    "    # Instead of dropping the Embarked column we replace NaN values\n",
    "    # with `S` denoting Southampton, the most common embarking\n",
    "    # location\n",
    "    X_full['Embarked'].fillna('S', inplace=True)\n",
    "\n",
    "    # We then use the get_dummies function to transform text\n",
    "    # and non-numerical values into binary categories.\n",
    "    X_dummies = pd.get_dummies(\n",
    "        X_full,\n",
    "        columns=['Sex', 'Cabin_mapped', 'Embarked'],\n",
    "        drop_first=True)\n",
    "\n",
    "    # We now have the cleaned data we can use in the assignment\n",
    "    X = X_dummies[:len(train)]\n",
    "    submission_X = X_dummies[len(train):]\n",
    "    y = train.Survived\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=.3, random_state=5, stratify=y)\n",
    "\n",
    "    return (X_train, y_train), (X_test, y_test), submission_X\n",
    "\n",
    "#get_better_titanic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "146cbdd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7910447761194029, 0.7422680412371134, 0.6990291262135923)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Section 2.1\n",
    "\n",
    "def rfc_train_test(X_train, t_train, X_test, t_test):\n",
    "    '''\n",
    "    Train a random forest classifier on (X_train, t_train)\n",
    "    and evaluate it on (X_test, t_test)\n",
    "    '''\n",
    "    rfc = RandomForestClassifier()\n",
    "    rfc.fit(X_train, t_train)\n",
    "    predictions = rfc.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(t_test, predictions)\n",
    "    precision = precision_score(t_test, predictions)\n",
    "    recall = recall_score(t_test, predictions)\n",
    "    \n",
    "    return accuracy, precision, recall\n",
    "\n",
    "(tr_X, tr_y), (tst_X, tst_y), _ = get_titanic()\n",
    "rfc_train_test(tr_X, tr_y, tst_X, tst_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fa4880c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8208955223880597, 0.8313253012048193, 0.6699029126213593)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Section 2.3\n",
    "\n",
    "def gb_train_test(X_train, t_train, X_test, t_test):\n",
    "    '''\n",
    "    Train a Gradient boosting classifier on (X_train, t_train)\n",
    "    and evaluate it on (X_test, t_test)\n",
    "    '''\n",
    "    gbc = GradientBoostingClassifier()\n",
    "    gbc.fit(X_train, t_train)\n",
    "    predictions = gbc.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(t_test, predictions)\n",
    "    precision = precision_score(t_test, predictions)\n",
    "    recall = recall_score(t_test, predictions)\n",
    "    \n",
    "    return accuracy, precision, recall\n",
    "\n",
    "(tr_X, tr_y), (tst_X, tst_y), _ = get_titanic()\n",
    "gb_train_test(tr_X, tr_y, tst_X, tst_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3db23660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 63, 'max_depth': 4, 'learning_rate': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Section 2.5\n",
    "\n",
    "def param_search(X, y):\n",
    "    '''\n",
    "    Perform randomized parameter search on the\n",
    "    gradient boosting classifier on the dataset (X, y)\n",
    "    '''\n",
    "    # Create the parameter grid\n",
    "    gb_param_grid = {\n",
    "        'n_estimators': [i for i in range(1, 101)],\n",
    "        'max_depth': [i for i in range(1, 51)],\n",
    "        'learning_rate': [i for i in np.linspace(1,1,200)]}\n",
    "    # Instantiate the regressor\n",
    "    gb = GradientBoostingClassifier()\n",
    "    # Perform random search\n",
    "    gb_random = RandomizedSearchCV(\n",
    "        param_distributions=gb_param_grid,\n",
    "        estimator=gb,\n",
    "        scoring=\"accuracy\",\n",
    "        verbose=0,\n",
    "        n_iter=50,\n",
    "        cv=4)\n",
    "    # Fit randomized_mse to the data\n",
    "    gb_random.fit(X, y)\n",
    "    # Print the best parameters and lowest RMSE\n",
    "    return gb_random.best_params_\n",
    "\n",
    "(tr_X, tr_y), (tst_X, tst_y), _ = get_titanic()\n",
    "best_params = param_search(tr_X, tr_y)\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f240468c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8246268656716418, 0.8111111111111111, 0.7087378640776699)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Section 2.6\n",
    "\n",
    "def gb_optimized_train_test(X_train, t_train, X_test, t_test):\n",
    "    '''\n",
    "    Train a gradient boosting classifier on (X_train, t_train)\n",
    "    and evaluate it on (X_test, t_test) with\n",
    "    your own optimized parameters\n",
    "    '''\n",
    "    best_params = param_search(X_train, t_train)\n",
    "    #print(best_params)\n",
    "    \n",
    "    gb_optimized = GradientBoostingClassifier(\n",
    "        n_estimators = best_params['n_estimators'],\n",
    "        max_depth = best_params['max_depth'],\n",
    "        learning_rate = best_params['learning_rate']\n",
    "    )\n",
    "    gb_optimized.fit(X_train, t_train)\n",
    "    \n",
    "    t_pred = gb_optimized.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(t_test, t_pred)\n",
    "    precision = precision_score(t_test, t_pred)\n",
    "    recall = recall_score(t_test, t_pred)\n",
    "    \n",
    "    return accuracy, precision, recall\n",
    "\n",
    "(tr_X, tr_y), (tst_X, tst_y), _ = get_titanic()\n",
    "gb_optimized_train_test(tr_X, tr_y, tst_X, tst_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeabdc5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
